# NLP Models and BERT

This project explores different NLP models and discusses the capabilities and applications of Google's BERT (Bidirectional Encoder Representations from Transformers) model.


## Task 22 - Capstone Task

## Email Categorisation

A model is developed to allocate emails to specific mail folders (e.g., work, friends, promotions, important) similar to Gmail's inbox tabs. This task can be accomplished using text classification techniques. Text classification involves assigning predefined categories or labels to a given text document (email) based on its content.

## Essay Grading Model

A model is designed to assist university professors or essay competition organisers in grading essay questions. This model utilises supervised learning techniques where the program learns from examples that have had the correct grades assigned to them beforehand. By comparing students' answers to previous or assigned correct answers, the model can assign appropriate grades to essay questions.


## Assistive Diagnosis Model for Doctors

A model is developed to provide assistive technology for doctors in making diagnoses. The model uses the patients' answers to questions posed by the doctor to provide probable diagnoses. This task can be accomplished using a question-answering model. The model leverages the patients' answers to the doctors' questions to generate multiple potential diagnoses, which the doctor can then evaluate and make decisions based on.

## Google's BERT (Bidirectional Encoder Representations from Transformers)

Google's BERT is a pre-trained language model that excels in understanding the context and meaning of language. It has become a popular choice for various natural language processing (NLP) tasks due to its ability to comprehend language in a highly sophisticated manner.

Key features of BERT include:

Contextual Understanding: BERT is pre-trained on a vast corpus of text data, enabling it to understand how language is used in different contexts. This contextual understanding empowers BERT to excel in a wide range of NLP tasks, such as question-answering, text classification, and language translation.
Transformer Architecture: BERT employs a transformer architecture, enabling it to process text bidirectionally, from both left to right and right to left. This comprehensive understanding of word and sentence relationships enhances its language comprehension capabilities. Additionally, BERT utilises context to interpret vague words or phrases and perform tasks like text completion effectively.
BERT has been utilised by Google in several applications:

Search: BERT enhances search results by comprehending the meaning behind search queries. This improves the relevance of search results by considering the context and nuances of the search terms.
Smart Reply: BERT powers Google's Smart Reply feature, similar to Gmail's auto-response suggestions. It suggests relevant responses to messages based on their content, allowing users to respond quickly and effortlessly without typing a response from scratch.
BERT's innovative technology has significantly advanced the field of NLP. Its contextual understanding and transformer architecture have made it a preferred choice for various applications. The model's impact can be observed in improved search results and efficient email response suggestions.

Conclusion

This project explores different NLP models, such as email categorisation, essay grading, and doctor's diagnosis assistant. Additionally, it highlights the capabilities and applications of Google's BERT model, which has revolutionised NLP tasks with its contextual understanding and transformer architecture.
