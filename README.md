# NLP Models and BERT

## Task 22 - Capstone Task

## Email Categorisation
A model that allocates which mail folder an email should be sent to (work, friends, promotions, important), like Gmail's inbox tabs.

**Answer**: Text classification would be used for email categorisation. This involves assigning pre-defined categories or labels to a given text document (email).

## Essay Grading Model
A model that helps decide what grade to award to an essay question. This can be used by a university professor who grades a lot of classes or essay competitions.

**Answer**: A supervised learning method would be used for this, as the program learns from examples that have had the right answers assigned to them beforehand. This will help grade the essay based on the students' answers against previous/assigned correct answers.

## Assistive Diagnosis Model for Doctors
A model that provides assistive technology for doctors to provide their diagnosis. Remember, doctors ask questions, so the model will use the patients' answers to provide a probable diagnosis for the doctor to weigh and make decisions.

**Answer**: A Question Answering model would help assist the doctor in providing multiple diagnoses depending on the patient's answers to the doctor's questions.

## Google's BERT (Bidirectional Encoder Representations from Transformers)
Google's BERT is a pre-trained language model that is capable of understanding the context and meaning of language to a high degree, which has made it a popular choice for a variety of NLP tasks.

BERT is capable of understanding language in a more complex manner than previous models, as it is pre-trained on a large corpus of text data, which allows it to understand how language is used in context. This allows BERT to perform a wide range of NLP tasks, including question-answering, text classification, and language translation, among others.

One of the key features of BERT is its use of a transformer architecture. This allows BERT to process text in both directions from left to right and right to left, which helps it to better understand the relationships between words and sentences. Additionally, BERT is able to use context to inform its understanding of the language, which makes it particularly effective at tasks such as text completion or understanding the meaning of vague words or phrases.

Google has used BERT in a variety of applications, including search, where it helps to deliver more relevant search results by understanding the meaning behind search queries. Additionally, BERT has been used in Google's Smart Reply feature, which is similar to the Gmail auto-response suggestions. Smart Reply suggests responses to messages based on their content, allowing users to quickly and easily respond to messages without having to type out a response.

Overall, BERT is an innovative technology that has significantly advanced the state of the art in NLP. Its ability to understand language in context has made it a popular choice for a wide range of applications, and its use of transformer architecture has helped to improve the accuracy of NLP models.
